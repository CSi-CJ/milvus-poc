"""
è§†é¢‘æ–‡ä»¶è§£æå™¨ - å¢å¼ºç‰ˆï¼Œæ”¯æŒå®Œæ•´çš„è§†é¢‘å¸§OCRæ–‡æœ¬æå–
"""

import os
import tempfile
from typing import Dict, Any, Optional, List
import logging

try:
    import cv2
except ImportError:
    cv2 = None

try:
    import ffmpeg
except ImportError:
    ffmpeg = None

from ..models import ParsedContent
from .base import BaseFileParser
from .audio_parser import AudioParser


class VideoParser(BaseFileParser):
    """è§†é¢‘æ–‡ä»¶è§£æå™¨ - å¢å¼ºç‰ˆ"""
    
    def __init__(self, enable_audio_extraction: bool = True, max_frames: int = 15, 
                 enable_enhanced_ocr: bool = True):
        super().__init__()
        self.enable_audio_extraction = enable_audio_extraction
        self.max_frames = max_frames
        self.enable_enhanced_ocr = enable_enhanced_ocr
        self.supported_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv'}
        self.audio_parser = AudioParser() if enable_audio_extraction else None
        
        # åˆå§‹åŒ–å¢å¼ºOCRå¤„ç†å™¨ - ä½¿ç”¨ChatGPTå¸§ç­–ç•¥æ–¹æ¡ˆ
        if enable_enhanced_ocr:
            try:
                from ..chatgpt_frame_strategy_processor import ChatGPTFrameStrategyProcessor
                self.enhanced_ocr = ChatGPTFrameStrategyProcessor()
                self.enhanced_ocr_type = 'chatgpt_frame_strategy'
                self.logger.info("âœ… ChatGPTå¸§ç­–ç•¥è§†é¢‘OCRå¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            except ImportError as e:
                self.logger.warning(f"ChatGPTå¸§ç­–ç•¥OCRå¤„ç†å™¨ä¸å¯ç”¨: {e}")
                self.enhanced_ocr = None
                self.enhanced_ocr_type = None
        else:
            self.enhanced_ocr = None
            self.enhanced_ocr_type = None
        
        if cv2 is None:
            self.logger.warning("OpenCV not installed. Video parsing will be limited.")
        
        if enable_audio_extraction and ffmpeg is None:
            self.logger.warning("ffmpeg-python not installed. Audio extraction will be disabled.")
    
    def can_parse(self, file_path: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦èƒ½è§£æè§†é¢‘æ–‡ä»¶"""
        ext = self._get_file_extension(file_path)
        return ext in self.supported_extensions
    
    def parse(self, file_path: str) -> ParsedContent:
        """è§£æè§†é¢‘æ–‡ä»¶"""
        self._validate_file(file_path)
        
        if cv2 is None:
            return self._create_error_content(
                file_path,
                "OpenCV not installed. Please install with: pip install opencv-python"
            )
        
        try:
            # æ‰“å¼€è§†é¢‘æ–‡ä»¶
            cap = cv2.VideoCapture(file_path)
            
            if not cap.isOpened():
                return self._create_error_content(file_path, "Failed to open video file")
            
            # è·å–è§†é¢‘ä¿¡æ¯
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            duration = frame_count / fps if fps > 0 else 0
            
            metadata = {
                'duration': duration,
                'fps': fps,
                'frame_count': frame_count,
                'width': width,
                'height': height,
                'resolution': f"{width}x{height}",
                'enhanced_ocr_enabled': self.enhanced_ocr is not None
            }
            
            # æå–å…³é”®å¸§
            key_frames = self._extract_key_frames(cap, frame_count)
            
            cap.release()
            
            # æå–éŸ³é¢‘
            audio_content = None
            text_content = None
            if self.enable_audio_extraction and self.audio_parser:
                audio_result = self._extract_audio(file_path)
                if audio_result:
                    audio_content = audio_result.get('audio_data')
                    text_content = audio_result.get('transcription')
                    if audio_result.get('metadata'):
                        metadata['audio'] = audio_result['metadata']
            
            # å¦‚æœæ²¡æœ‰éŸ³é¢‘è½¬å½•å†…å®¹ï¼Œæˆ–è€…éŸ³é¢‘è½¬å½•å†…å®¹å¾ˆå°‘ï¼Œä½¿ç”¨å¢å¼ºOCRæå–è§†é¢‘å¸§æ–‡æœ¬
            if self.enhanced_ocr and key_frames:
                if not text_content or len(text_content.strip()) < 50:
                    self.logger.info("ğŸ”„ éŸ³é¢‘è½¬å½•å†…å®¹ä¸è¶³ï¼Œå¯ç”¨å¢å¼ºè§†é¢‘OCRæå–...")
                    
                    # æ ¹æ®ä¸åŒçš„OCRå¤„ç†å™¨ç±»å‹ä½¿ç”¨ä¸åŒçš„ç­–ç•¥
                    if hasattr(self, 'enhanced_ocr_type') and self.enhanced_ocr_type == 'chatgpt_frame_strategy':
                        self.logger.info("ğŸš€ ä½¿ç”¨ChatGPTå®Œæ•´å¸§ç­–ç•¥å¤„ç†è§†é¢‘...")
                        ocr_results = self.enhanced_ocr.extract_frames_with_chatgpt_strategy(file_path)
                    elif hasattr(self, 'enhanced_ocr_type') and self.enhanced_ocr_type == 'pure_chatgpt':
                        self.logger.info("ğŸš€ ä½¿ç”¨çº¯ChatGPTç­–ç•¥å¤„ç†è§†é¢‘...")
                        ocr_results = self.enhanced_ocr.process_video_with_pure_chatgpt_strategy(file_path)
                    elif hasattr(self, 'enhanced_ocr_type') and self.enhanced_ocr_type == 'enhanced_chatgpt':
                        self.logger.info("ğŸš€ ä½¿ç”¨å¢å¼ºChatGPTç­–ç•¥å¤„ç†è§†é¢‘...")
                        ocr_results = self.enhanced_ocr.process_video_with_enhanced_strategy(file_path)
                    else:
                        # ä½¿ç”¨ä¼ ç»Ÿçš„å¸§å¤„ç†æ–¹å¼
                        ocr_results = self.enhanced_ocr.extract_comprehensive_text_from_frames(key_frames)
                    
                    # åˆå¹¶æ‰€æœ‰å¸§çš„OCRæ–‡æœ¬
                    frame_texts = []
                    total_confidence = 0
                    successful_frames = 0
                    
                    for result in ocr_results:
                        if result['text'].strip():
                            # ChatGPTå¸§ç­–ç•¥åŒ…å«æ—¶é—´æˆ³å’Œä¼˜å…ˆçº§ä¿¡æ¯
                            if 'timestamp' in result and 'priority_score' in result:
                                frame_texts.append(f"[å¸§ {result['frame_number']} - {result['timestamp']:.1f}s - ä¼˜å…ˆçº§:{result['priority_score']:.1f}] {result['text']}")
                            else:
                                frame_texts.append(f"[å¸§ {result['frame_number']}] {result['text']}")
                            total_confidence += result['confidence']
                            successful_frames += 1
                    
                    if frame_texts:
                        ocr_text = '\n\n'.join(frame_texts)
                        avg_confidence = total_confidence / successful_frames if successful_frames > 0 else 0
                        
                        # å¦‚æœOCRæ–‡æœ¬æ¯”éŸ³é¢‘è½¬å½•æ›´ä¸°å¯Œï¼Œä½¿ç”¨OCRæ–‡æœ¬
                        if not text_content or len(ocr_text) > len(text_content) * 2:
                            text_content = ocr_text
                            extraction_method = getattr(self, 'enhanced_ocr_type', 'enhanced_multi_engine')
                            metadata['ocr_extraction'] = {
                                'successful_frames': successful_frames,
                                'total_frames': len(ocr_results),
                                'average_confidence': avg_confidence,
                                'extraction_method': extraction_method
                            }
                            
                            # æ·»åŠ ChatGPTå¸§ç­–ç•¥çš„é¢å¤–ä¿¡æ¯
                            if self.enhanced_ocr_type == 'chatgpt_frame_strategy':
                                metadata['frame_strategy'] = {
                                    'similarity_filtering': True,
                                    'priority_ranking': True,
                                    'fps_extraction': '1_fps'
                                }
                            
                            self.logger.info(f"âœ… å¢å¼ºOCRæå–æˆåŠŸ: {successful_frames}/{len(ocr_results)} å¸§, å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
                        else:
                            # å°†OCRæ–‡æœ¬ä½œä¸ºè¡¥å……ä¿¡æ¯æ·»åŠ åˆ°å…ƒæ•°æ®
                            metadata['supplementary_ocr'] = ocr_text
                            self.logger.info("ğŸ“ OCRæ–‡æœ¬ä½œä¸ºè¡¥å……ä¿¡æ¯ä¿å­˜")
                    else:
                        self.logger.warning("âš ï¸  å¢å¼ºOCRæœªèƒ½æå–åˆ°æ–‡æœ¬å†…å®¹")
            
            return ParsedContent(
                text_content=text_content,
                image_content=key_frames if key_frames else None,
                audio_content=audio_content,
                metadata=metadata,
                file_type=self._get_file_extension(file_path)
            )
            
        except Exception as e:
            self.logger.error(f"Error parsing video {file_path}: {e}")
            return self._create_error_content(file_path, str(e))
    
    def _extract_key_frames(self, cap, frame_count: int) -> Optional[List[bytes]]:
        """æ™ºèƒ½æå–å…³é”®å¸§ - åŸºäºåœºæ™¯å˜åŒ–å’Œè´¨é‡ä¼˜åŒ–"""
        try:
            frames = []
            fps = cap.get(cv2.CAP_PROP_FPS)
            
            # å¯ç”¨åœºæ™¯æ£€æµ‹å’Œè´¨é‡ä¼˜åŒ–
            scene_threshold = 0.3
            min_frame_interval = 30
            high_quality_frames = True
            
            # å¦‚æœå¯ç”¨åœºæ™¯æ£€æµ‹
            if scene_threshold > 0 and frame_count > min_frame_interval:
                selected_frames = self._detect_scene_changes(cap, frame_count, fps, scene_threshold, min_frame_interval)
            else:
                # å›é€€åˆ°å‡åŒ€é‡‡æ ·
                selected_frames = self._uniform_sampling(frame_count)
            
            self.logger.info(f"Selected {len(selected_frames)} frames for extraction")
            
            # æå–é€‰å®šçš„å¸§
            for frame_idx in selected_frames:
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                
                if ret:
                    # é«˜è´¨é‡å¤„ç†
                    if high_quality_frames:
                        frame = self._enhance_frame_quality(frame)
                    
                    # è½¬æ¢ä¸ºé«˜è´¨é‡PNG
                    encode_params = [cv2.IMWRITE_PNG_COMPRESSION, 3]  # è¾ƒä½å‹ç¼©ï¼Œä¿æŒè´¨é‡
                    success, buffer = cv2.imencode('.png', frame, encode_params)
                    
                    if success:
                        frame_bytes = buffer.tobytes()
                        frames.append(frame_bytes)
                        
                        # è®°å½•å¸§ä¿¡æ¯
                        timestamp = frame_idx / fps if fps > 0 else 0
                        self.logger.debug(f"Extracted frame {frame_idx} at {timestamp:.2f}s, size: {len(frame_bytes)} bytes")
                    else:
                        self.logger.warning(f"Failed to encode frame {frame_idx}")
                else:
                    self.logger.warning(f"Failed to read frame {frame_idx}")
            
            self.logger.info(f"Successfully extracted {len(frames)} high-quality key frames")
            return frames if frames else None
            
        except Exception as e:
            self.logger.warning(f"Failed to extract key frames: {e}")
            return None
    
    def _detect_scene_changes(self, cap, frame_count: int, fps: float, scene_threshold: float, min_frame_interval: int) -> List[int]:
        """æ£€æµ‹åœºæ™¯å˜åŒ–ç‚¹"""
        try:
            import numpy as np
            
            scene_frames = [0]  # æ€»æ˜¯åŒ…å«ç¬¬ä¸€å¸§
            prev_hist = None
            last_selected = 0
            
            # è®¡ç®—æ£€æŸ¥é—´éš”ï¼ˆé¿å…è¿‡äºé¢‘ç¹çš„æ£€æŸ¥ï¼‰
            check_interval = max(1, int(fps * 0.5))  # æ¯0.5ç§’æ£€æŸ¥ä¸€æ¬¡
            
            for frame_idx in range(0, frame_count, check_interval):
                if len(scene_frames) >= self.max_frames:
                    break
                
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                
                if not ret:
                    continue
                
                # è®¡ç®—ç›´æ–¹å›¾
                hist = self._calculate_histogram(frame)
                
                if prev_hist is not None:
                    # è®¡ç®—ç›´æ–¹å›¾å·®å¼‚
                    similarity = cv2.compareHist(prev_hist, hist, cv2.HISTCMP_CORREL)
                    
                    # å¦‚æœç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯åœºæ™¯å˜åŒ–
                    if similarity < (1 - scene_threshold):
                        # ç¡®ä¿ä¸ä¸Šä¸€ä¸ªé€‰ä¸­å¸§æœ‰è¶³å¤Ÿé—´éš”
                        if frame_idx - last_selected >= min_frame_interval:
                            scene_frames.append(frame_idx)
                            last_selected = frame_idx
                            self.logger.debug(f"Scene change detected at frame {frame_idx}, similarity: {similarity:.3f}")
                
                prev_hist = hist
            
            # å¦‚æœåœºæ™¯å˜åŒ–ç‚¹å¤ªå°‘ï¼Œè¡¥å……ä¸€äº›å‡åŒ€åˆ†å¸ƒçš„å¸§
            if len(scene_frames) < self.max_frames // 2:
                uniform_frames = self._uniform_sampling(frame_count)
                for frame_idx in uniform_frames:
                    if frame_idx not in scene_frames and len(scene_frames) < self.max_frames:
                        scene_frames.append(frame_idx)
            
            # ç¡®ä¿åŒ…å«æœ€åä¸€å¸§
            if frame_count - 1 not in scene_frames and len(scene_frames) < self.max_frames:
                scene_frames.append(frame_count - 1)
            
            scene_frames.sort()
            return scene_frames[:self.max_frames]
            
        except Exception as e:
            self.logger.warning(f"Scene detection failed: {e}, falling back to uniform sampling")
            return self._uniform_sampling(frame_count)
    
    def _calculate_histogram(self, frame):
        """è®¡ç®—å¸§çš„é¢œè‰²ç›´æ–¹å›¾"""
        import numpy as np
        
        # è½¬æ¢ä¸ºHSVè‰²å½©ç©ºé—´ï¼Œå¯¹å…‰ç…§å˜åŒ–æ›´é²æ£’
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        
        # è®¡ç®—Hå’ŒSé€šé“çš„ç›´æ–¹å›¾
        hist = cv2.calcHist([hsv], [0, 1], None, [50, 60], [0, 180, 0, 256])
        
        # å½’ä¸€åŒ–
        cv2.normalize(hist, hist, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)
        
        return hist
    
    def _uniform_sampling(self, frame_count: int) -> List[int]:
        """å‡åŒ€é‡‡æ ·å¸§"""
        if frame_count <= self.max_frames:
            return list(range(frame_count))
        
        frame_interval = frame_count // self.max_frames
        return list(range(0, frame_count, frame_interval))[:self.max_frames]
    
    def _enhance_frame_quality(self, frame):
        """å¢å¼ºå¸§è´¨é‡ä»¥æå‡OCRæ•ˆæœ"""
        try:
            import numpy as np
            
            # 1. å»å™ª
            frame = cv2.bilateralFilter(frame, 9, 75, 75)
            
            # 2. é”åŒ–
            kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
            frame = cv2.filter2D(frame, -1, kernel)
            
            # 3. å¯¹æ¯”åº¦å¢å¼º
            lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)
            l, a, b = cv2.split(lab)
            
            # CLAHE (å¯¹æ¯”åº¦é™åˆ¶è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ–)
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
            l = clahe.apply(l)
            
            frame = cv2.merge([l, a, b])
            frame = cv2.cvtColor(frame, cv2.COLOR_LAB2BGR)
            
            return frame
            
        except Exception as e:
            self.logger.warning(f"Frame enhancement failed: {e}")
            return frame
    
    def _extract_audio(self, file_path: str) -> Optional[Dict[str, Any]]:
        """æå–éŸ³é¢‘è½¨é“"""
        if ffmpeg is None or self.audio_parser is None:
            return None
        
        temp_audio_file = None
        try:
            # åˆ›å»ºä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                temp_audio_file = temp_file.name
            
            # ä½¿ç”¨ ffmpeg æå–éŸ³é¢‘
            (
                ffmpeg
                .input(file_path)
                .output(temp_audio_file, acodec='pcm_s16le', ac=1, ar='16000')
                .overwrite_output()
                .run(quiet=True)
            )
            
            # ä½¿ç”¨éŸ³é¢‘è§£æå™¨å¤„ç†æå–çš„éŸ³é¢‘
            audio_result = self.audio_parser.parse(temp_audio_file)
            
            if audio_result.has_content():
                return {
                    'audio_data': audio_result.audio_content,
                    'transcription': audio_result.text_content,
                    'metadata': audio_result.metadata
                }
            else:
                return None
                
        except Exception as e:
            self.logger.warning(f"Failed to extract audio from video: {e}")
            return None
        
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_audio_file and os.path.exists(temp_audio_file):
                try:
                    os.unlink(temp_audio_file)
                except:
                    pass