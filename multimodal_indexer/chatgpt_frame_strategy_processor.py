#!/usr/bin/env python3
"""
ChatGPTæ¨èçš„è§†é¢‘å¸§æå–ç­–ç•¥å¤„ç†å™¨
é‡‡ç”¨ 1 FPS æŠ½å¸§ + ç›¸ä¼¼åº¦è¿‡æ»¤ + é‡ç‚¹å¸§è¯†åˆ«çš„å®Œæ•´æ–¹æ¡ˆ
"""

import cv2
import numpy as np
from typing import List, Dict, Any, Tuple
import logging
from PIL import Image, ImageEnhance
import io
import hashlib
from skimage.metrics import structural_similarity as ssim

class ChatGPTFrameStrategyProcessor:
    """åŸºäºChatGPTæ¨èçš„å®Œæ•´å¸§æå–ç­–ç•¥å¤„ç†å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.reader = self._initialize_easyocr()
        
    def _initialize_easyocr(self):
        """åˆå§‹åŒ–EasyOCR"""
        try:
            import easyocr
            reader = easyocr.Reader(['ch_sim', 'en'], verbose=False)
            self.logger.info("âœ… EasyOCRåˆå§‹åŒ–æˆåŠŸ")
            return reader
        except ImportError:
            self.logger.error("âŒ EasyOCRä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install easyocr")
            return None
    
    def extract_frames_with_chatgpt_strategy(self, video_path: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ChatGPTæ¨èçš„å®Œæ•´ç­–ç•¥æå–è§†é¢‘å¸§"""
        if not self.reader:
            return []
        
        self.logger.info("ğŸš€ å¼€å§‹ChatGPTå®Œæ•´å¸§æå–ç­–ç•¥...")
        
        # æ­¥éª¤1: 1 FPS æŠ½å¸§ç­–ç•¥
        raw_frames = self._extract_1fps_frames(video_path)
        if not raw_frames:
            return []
        
        self.logger.info(f"ğŸ“Š 1 FPSç­–ç•¥æå–äº† {len(raw_frames)} å¸§")
        
        # æ­¥éª¤2: ç›¸ä¼¼åº¦è¿‡æ»¤
        filtered_frames = self._filter_similar_frames(raw_frames)
        self.logger.info(f"ğŸ” ç›¸ä¼¼åº¦è¿‡æ»¤åå‰©ä½™ {len(filtered_frames)} å¸§ (å‡å°‘äº† {len(raw_frames) - len(filtered_frames)} å¸§)")
        
        # æ­¥éª¤3: é‡ç‚¹å¸§è¯†åˆ«å’Œæ’åº
        prioritized_frames = self._prioritize_frames(filtered_frames)
        self.logger.info(f"â­ é‡ç‚¹å¸§è¯†åˆ«å®Œæˆï¼ŒæŒ‰é‡è¦æ€§æ’åº")
        
        # æ­¥éª¤4: OCRæ–‡æœ¬æå–
        ocr_results = self._extract_text_from_prioritized_frames(prioritized_frames)
        
        return ocr_results
    
    def _extract_1fps_frames(self, video_path: str) -> List[Dict[str, Any]]:
        """æ­¥éª¤1: é‡‡ç”¨ 1 FPS æŠ½å¸§ç­–ç•¥"""
        try:
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                self.logger.error(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
                return []
            
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = frame_count / fps if fps > 0 else 0
            
            self.logger.info(f"ğŸ“¹ è§†é¢‘ä¿¡æ¯: {duration:.1f}ç§’, {fps:.1f} FPS, {frame_count} æ€»å¸§æ•°")
            
            frames = []
            frame_interval = int(fps)  # 1 FPS = æ¯ç§’1å¸§
            
            for frame_idx in range(0, frame_count, frame_interval):
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                
                if ret:
                    # è½¬æ¢ä¸ºé«˜è´¨é‡PNG
                    success, buffer = cv2.imencode('.png', frame, [cv2.IMWRITE_PNG_COMPRESSION, 1])
                    if success:
                        timestamp = frame_idx / fps if fps > 0 else 0
                        frames.append({
                            'frame_number': len(frames),
                            'original_frame_idx': frame_idx,
                            'timestamp': timestamp,
                            'frame_data': buffer.tobytes(),
                            'frame_array': frame.copy()  # ä¿å­˜ç”¨äºç›¸ä¼¼åº¦è®¡ç®—
                        })
                        self.logger.debug(f"æå–å¸§ {frame_idx} (æ—¶é—´: {timestamp:.1f}s)")
            
            cap.release()
            self.logger.info(f"âœ… 1 FPSç­–ç•¥å®Œæˆï¼Œæå– {len(frames)} å¸§")
            return frames
            
        except Exception as e:
            self.logger.error(f"âŒ 1 FPSå¸§æå–å¤±è´¥: {e}")
            return []
    
    def _filter_similar_frames(self, frames: List[Dict[str, Any]], 
                             ssim_threshold: float = 0.95, 
                             phash_threshold: int = 5) -> List[Dict[str, Any]]:
        """æ­¥éª¤2: è¿‡æ»¤é‡å¤æˆ–ç›¸ä¼¼å¸§"""
        if len(frames) <= 1:
            return frames
        
        filtered_frames = [frames[0]]  # æ€»æ˜¯ä¿ç•™ç¬¬ä¸€å¸§
        
        for i, current_frame in enumerate(frames[1:], 1):
            is_similar = False
            
            # ä¸å·²é€‰æ‹©çš„å¸§è¿›è¡Œç›¸ä¼¼åº¦æ¯”è¾ƒ
            for selected_frame in filtered_frames:
                # æ–¹æ³•1: SSIMç»“æ„ç›¸ä¼¼åº¦
                similarity = self._calculate_ssim(
                    selected_frame['frame_array'], 
                    current_frame['frame_array']
                )
                
                if similarity > ssim_threshold:
                    is_similar = True
                    self.logger.debug(f"å¸§ {i} ä¸å·²é€‰å¸§ç›¸ä¼¼ (SSIM: {similarity:.3f})")
                    break
                
                # æ–¹æ³•2: æ„ŸçŸ¥å“ˆå¸Œ
                hash_distance = self._calculate_phash_distance(
                    selected_frame['frame_array'], 
                    current_frame['frame_array']
                )
                
                if hash_distance < phash_threshold:
                    is_similar = True
                    self.logger.debug(f"å¸§ {i} ä¸å·²é€‰å¸§ç›¸ä¼¼ (Hashè·ç¦»: {hash_distance})")
                    break
            
            if not is_similar:
                filtered_frames.append(current_frame)
                self.logger.debug(f"ä¿ç•™å¸§ {i} (æ—¶é—´: {current_frame['timestamp']:.1f}s)")
        
        # æ¸…ç†frame_arrayä»¥èŠ‚çœå†…å­˜
        for frame in filtered_frames:
            if 'frame_array' in frame:
                del frame['frame_array']
        
        return filtered_frames
    
    def _calculate_ssim(self, frame1: np.ndarray, frame2: np.ndarray) -> float:
        """è®¡ç®—ä¸¤å¸§ä¹‹é—´çš„ç»“æ„ç›¸ä¼¼åº¦"""
        try:
            # è½¬æ¢ä¸ºç°åº¦å›¾
            gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
            gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)
            
            # è°ƒæ•´å°ºå¯¸ä»¥æé«˜è®¡ç®—é€Ÿåº¦
            height, width = gray1.shape
            if width > 640:
                scale = 640 / width
                new_width = int(width * scale)
                new_height = int(height * scale)
                gray1 = cv2.resize(gray1, (new_width, new_height))
                gray2 = cv2.resize(gray2, (new_width, new_height))
            
            # è®¡ç®—SSIM
            similarity = ssim(gray1, gray2)
            return similarity
            
        except Exception as e:
            self.logger.warning(f"SSIMè®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_phash_distance(self, frame1: np.ndarray, frame2: np.ndarray) -> int:
        """è®¡ç®—ä¸¤å¸§ä¹‹é—´çš„æ„ŸçŸ¥å“ˆå¸Œè·ç¦»"""
        try:
            def perceptual_hash(frame):
                # è½¬æ¢ä¸ºç°åº¦å›¾å¹¶è°ƒæ•´å¤§å°
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                resized = cv2.resize(gray, (8, 8))
                
                # è®¡ç®—å¹³å‡å€¼
                avg = resized.mean()
                
                # ç”Ÿæˆå“ˆå¸Œ
                hash_bits = []
                for row in resized:
                    for pixel in row:
                        hash_bits.append('1' if pixel > avg else '0')
                
                return ''.join(hash_bits)
            
            hash1 = perceptual_hash(frame1)
            hash2 = perceptual_hash(frame2)
            
            # è®¡ç®—æ±‰æ˜è·ç¦»
            distance = sum(c1 != c2 for c1, c2 in zip(hash1, hash2))
            return distance
            
        except Exception as e:
            self.logger.warning(f"æ„ŸçŸ¥å“ˆå¸Œè®¡ç®—å¤±è´¥: {e}")
            return 100  # è¿”å›å¤§å€¼è¡¨ç¤ºä¸ç›¸ä¼¼
    
    def _prioritize_frames(self, frames: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ­¥éª¤3: é‡ç‚¹å¸§è¯†åˆ«å’Œä¼˜å…ˆçº§æ’åº"""
        for frame in frames:
            score = 0
            
            # åŠ è½½å¸§æ•°æ®è¿›è¡Œåˆ†æ
            frame_array = self._load_frame_from_bytes(frame['frame_data'])
            if frame_array is None:
                frame['priority_score'] = 0
                continue
            
            # è¯„åˆ†æ ‡å‡†1: æ–‡å­—å¯†åº¦ä¼°ç®—
            text_density = self._estimate_text_density(frame_array)
            score += text_density * 3  # æ–‡å­—å¯†åº¦æƒé‡æœ€é«˜
            
            # è¯„åˆ†æ ‡å‡†2: UIå…ƒç´ æ•°é‡
            ui_elements = self._count_ui_elements(frame_array)
            score += ui_elements * 2
            
            # è¯„åˆ†æ ‡å‡†3: å¯¹æ¯”åº¦å’Œæ¸…æ™°åº¦
            clarity = self._assess_frame_clarity(frame_array)
            score += clarity * 1.5
            
            # è¯„åˆ†æ ‡å‡†4: è¾¹ç¼˜å¯†åº¦ï¼ˆUIç•Œé¢é€šå¸¸è¾¹ç¼˜ä¸°å¯Œï¼‰
            edge_density = self._calculate_edge_density(frame_array)
            score += edge_density * 1
            
            frame['priority_score'] = score
            frame['text_density'] = text_density
            frame['ui_elements'] = ui_elements
            frame['clarity'] = clarity
            frame['edge_density'] = edge_density
            
            self.logger.debug(f"å¸§ {frame['frame_number']} è¯„åˆ†: {score:.2f} "
                            f"(æ–‡å­—:{text_density:.2f}, UI:{ui_elements:.2f}, "
                            f"æ¸…æ™°åº¦:{clarity:.2f}, è¾¹ç¼˜:{edge_density:.2f})")
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        prioritized = sorted(frames, key=lambda x: x['priority_score'], reverse=True)
        
        self.logger.info("ğŸ† å¸§ä¼˜å…ˆçº§æ’åºå®Œæˆ:")
        for i, frame in enumerate(prioritized[:5]):  # æ˜¾ç¤ºå‰5ä¸ª
            self.logger.info(f"  {i+1}. å¸§{frame['frame_number']} (æ—¶é—´:{frame['timestamp']:.1f}s) "
                           f"è¯„åˆ†:{frame['priority_score']:.2f}")
        
        return prioritized
    
    def _load_frame_from_bytes(self, frame_data: bytes) -> np.ndarray:
        """ä»å­—èŠ‚æ•°æ®åŠ è½½å¸§"""
        try:
            image = Image.open(io.BytesIO(frame_data))
            return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        except Exception as e:
            self.logger.warning(f"å¸§æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
    
    def _estimate_text_density(self, frame: np.ndarray) -> float:
        """ä¼°ç®—å¸§ä¸­çš„æ–‡å­—å¯†åº¦"""
        try:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # ä½¿ç”¨å½¢æ€å­¦æ“ä½œæ£€æµ‹æ–‡å­—åŒºåŸŸ
            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
            
            # è¾¹ç¼˜æ£€æµ‹
            edges = cv2.Canny(gray, 50, 150)
            
            # å½¢æ€å­¦é—­è¿ç®—è¿æ¥æ–‡å­—
            closed = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)
            
            # è®¡ç®—æ–‡å­—åŒºåŸŸæ¯”ä¾‹
            text_pixels = np.sum(closed > 0)
            total_pixels = closed.shape[0] * closed.shape[1]
            
            density = text_pixels / total_pixels
            return min(density * 10, 10)  # å½’ä¸€åŒ–åˆ°0-10
            
        except Exception as e:
            self.logger.warning(f"æ–‡å­—å¯†åº¦ä¼°ç®—å¤±è´¥: {e}")
            return 0
    
    def _count_ui_elements(self, frame: np.ndarray) -> float:
        """è®¡ç®—UIå…ƒç´ æ•°é‡"""
        try:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # æ£€æµ‹çŸ©å½¢åŒºåŸŸï¼ˆæŒ‰é’®ã€å¡ç‰‡ç­‰ï¼‰
            edges = cv2.Canny(gray, 50, 150)
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            ui_count = 0
            for contour in contours:
                area = cv2.contourArea(contour)
                if 100 < area < 50000:  # è¿‡æ»¤å¤ªå°æˆ–å¤ªå¤§çš„åŒºåŸŸ
                    # æ£€æŸ¥æ˜¯å¦æ¥è¿‘çŸ©å½¢
                    peri = cv2.arcLength(contour, True)
                    approx = cv2.approxPolyDP(contour, 0.02 * peri, True)
                    if len(approx) >= 4:  # ç±»ä¼¼çŸ©å½¢çš„å½¢çŠ¶
                        ui_count += 1
            
            return min(ui_count / 5, 10)  # å½’ä¸€åŒ–åˆ°0-10
            
        except Exception as e:
            self.logger.warning(f"UIå…ƒç´ è®¡æ•°å¤±è´¥: {e}")
            return 0
    
    def _assess_frame_clarity(self, frame: np.ndarray) -> float:
        """è¯„ä¼°å¸§çš„æ¸…æ™°åº¦"""
        try:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯ç®—å­è®¡ç®—æ¸…æ™°åº¦
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            
            # å½’ä¸€åŒ–åˆ°0-10
            clarity = min(laplacian_var / 100, 10)
            return clarity
            
        except Exception as e:
            self.logger.warning(f"æ¸…æ™°åº¦è¯„ä¼°å¤±è´¥: {e}")
            return 0
    
    def _calculate_edge_density(self, frame: np.ndarray) -> float:
        """è®¡ç®—è¾¹ç¼˜å¯†åº¦"""
        try:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            edges = cv2.Canny(gray, 50, 150)
            
            edge_pixels = np.sum(edges > 0)
            total_pixels = edges.shape[0] * edges.shape[1]
            
            density = edge_pixels / total_pixels
            return min(density * 20, 10)  # å½’ä¸€åŒ–åˆ°0-10
            
        except Exception as e:
            self.logger.warning(f"è¾¹ç¼˜å¯†åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0
    
    def _extract_text_from_prioritized_frames(self, frames: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ­¥éª¤4: ä»ä¼˜å…ˆçº§æ’åºçš„å¸§ä¸­æå–OCRæ–‡æœ¬"""
        results = []
        
        for frame in frames:
            self.logger.info(f"ğŸ”„ å¤„ç†ä¼˜å…ˆçº§å¸§ {frame['frame_number']} "
                           f"(è¯„åˆ†: {frame['priority_score']:.2f}, æ—¶é—´: {frame['timestamp']:.1f}s)")
            
            try:
                # å¢å¼ºå¸§è´¨é‡
                enhanced_frame_data = self._enhance_frame_for_ocr(frame['frame_data'])
                
                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                image = Image.open(io.BytesIO(enhanced_frame_data))
                image_array = np.array(image)
                
                # ä½¿ç”¨EasyOCRè¯†åˆ«
                ocr_results = self.reader.readtext(image_array)
                
                # æå–æ–‡å­—å’Œç½®ä¿¡åº¦
                texts = []
                confidences = []
                
                for (bbox, text, prob) in ocr_results:
                    if prob > 0.5:  # ç½®ä¿¡åº¦é˜ˆå€¼
                        texts.append(text)
                        confidences.append(prob)
                        self.logger.debug(f"è¯†åˆ«æ–‡å­—: {text} (ç½®ä¿¡åº¦: {prob:.3f})")
                
                # åˆå¹¶æ–‡æœ¬
                combined_text = '\n'.join(texts)
                avg_confidence = np.mean(confidences) if confidences else 0.0
                
                result = {
                    'frame_number': frame['frame_number'],
                    'timestamp': frame['timestamp'],
                    'text': combined_text,
                    'confidence': avg_confidence,
                    'priority_score': frame['priority_score'],
                    'text_density': frame.get('text_density', 0),
                    'ui_elements': frame.get('ui_elements', 0),
                    'clarity': frame.get('clarity', 0),
                    'engine': 'easyocr_chatgpt_strategy',
                    'method': 'chatgpt_frame_strategy'
                }
                
                results.append(result)
                
                self.logger.info(f"âœ… å¸§ {frame['frame_number']} å®Œæˆï¼Œ"
                               f"æå–æ–‡æœ¬: {len(combined_text)} å­—ç¬¦ï¼Œç½®ä¿¡åº¦: {avg_confidence:.3f}")
                
            except Exception as e:
                self.logger.error(f"âŒ å¸§ {frame['frame_number']} å¤„ç†å¤±è´¥: {e}")
                results.append({
                    'frame_number': frame['frame_number'],
                    'timestamp': frame['timestamp'],
                    'text': '',
                    'confidence': 0.0,
                    'priority_score': frame.get('priority_score', 0),
                    'engine': 'easyocr_chatgpt_strategy',
                    'error': str(e)
                })
        
        return results
    
    def _enhance_frame_for_ocr(self, frame_data: bytes) -> bytes:
        """å¢å¼ºå¸§è´¨é‡ä»¥æå‡OCRæ•ˆæœ"""
        try:
            # è½¬æ¢ä¸ºOpenCVæ ¼å¼
            image = Image.open(io.BytesIO(frame_data))
            cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
            
            # 1. å°ºå¯¸ä¼˜åŒ–
            height, width = cv_image.shape[:2]
            if width < 1920:
                scale_factor = min(2.5, 1920 / width)
                new_width = int(width * scale_factor)
                new_height = int(height * scale_factor)
                cv_image = cv2.resize(cv_image, (new_width, new_height), interpolation=cv2.INTER_LANCZOS4)
            
            # 2. å»å™ª
            denoised = cv2.bilateralFilter(cv_image, 9, 75, 75)
            
            # 3. é”åŒ–
            kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
            sharpened = cv2.filter2D(denoised, -1, kernel)
            
            # 4. å¯¹æ¯”åº¦å¢å¼º
            lab = cv2.cvtColor(sharpened, cv2.COLOR_BGR2LAB)
            l, a, b = cv2.split(lab)
            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
            l = clahe.apply(l)
            enhanced = cv2.merge([l, a, b])
            enhanced = cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)
            
            # 5. PILä¼˜åŒ–
            pil_image = Image.fromarray(cv2.cvtColor(enhanced, cv2.COLOR_BGR2RGB))
            
            # å¯¹æ¯”åº¦å¢å¼º
            enhancer = ImageEnhance.Contrast(pil_image)
            pil_image = enhancer.enhance(1.3)
            
            # é”åº¦å¢å¼º
            enhancer = ImageEnhance.Sharpness(pil_image)
            pil_image = enhancer.enhance(1.4)
            
            # è½¬æ¢å›å­—èŠ‚æ•°æ®
            enhanced_cv = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
            success, buffer = cv2.imencode('.png', enhanced_cv, [cv2.IMWRITE_PNG_COMPRESSION, 1])
            
            if success:
                return buffer.tobytes()
            else:
                return frame_data
                
        except Exception as e:
            self.logger.warning(f"å¸§å¢å¼ºå¤±è´¥: {e}")
            return frame_data


def test_chatgpt_frame_strategy():
    """æµ‹è¯•ChatGPTå¸§æå–ç­–ç•¥"""
    import os
    
    print("ğŸ§ª æµ‹è¯•ChatGPTå®Œæ•´å¸§æå–ç­–ç•¥")
    print("="*60)
    
    processor = ChatGPTFrameStrategyProcessor()
    
    if not processor.reader:
        print("âŒ EasyOCRä¸å¯ç”¨ï¼Œè¯·å…ˆå®‰è£…: pip install easyocr")
        return
    
    # æµ‹è¯•è§†é¢‘æ–‡ä»¶
    test_video = "./files/ä¸ªæ€§åŒ–æ¨è.mp4"
    
    if not os.path.exists(test_video):
        print(f"âŒ æµ‹è¯•è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {test_video}")
        return
    
    try:
        print(f"ğŸ”„ å¼€å§‹å¤„ç†è§†é¢‘: {test_video}")
        results = processor.extract_frames_with_chatgpt_strategy(test_video)
        
        print(f"\nğŸ“‹ ChatGPTå®Œæ•´ç­–ç•¥OCRç»“æœ:")
        print("="*60)
        
        total_text_length = 0
        successful_frames = 0
        
        for result in results:
            frame_num = result['frame_number']
            text = result['text']
            confidence = result['confidence']
            priority = result.get('priority_score', 0)
            timestamp = result.get('timestamp', 0)
            
            print(f"\nğŸ–¼ï¸  å¸§ {frame_num} (æ—¶é—´: {timestamp:.1f}s, ä¼˜å…ˆçº§: {priority:.2f}):")
            print(f"   ç­–ç•¥: ChatGPTå®Œæ•´å¸§æå–ç­–ç•¥")
            print(f"   ç½®ä¿¡åº¦: {confidence:.3f}")
            print(f"   æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
            
            if text:
                print(f"   ğŸ“ æå–æ–‡æœ¬:")
                print("   " + "-" * 50)
                for line in text.split('\n'):
                    if line.strip():
                        print(f"   {line}")
                print("   " + "-" * 50)
                total_text_length += len(text)
                successful_frames += 1
            else:
                print("   âš ï¸  æœªæå–åˆ°æ–‡æœ¬")
        
        print(f"\nğŸ“Š æ€»ç»“:")
        print(f"   å¤„ç†å¸§æ•°: {len(results)}")
        print(f"   æˆåŠŸæå–æ–‡æœ¬çš„å¸§æ•°: {successful_frames}")
        print(f"   æ€»æ–‡æœ¬é•¿åº¦: {total_text_length} å­—ç¬¦")
        print(f"   å¹³å‡æ¯å¸§æ–‡æœ¬é•¿åº¦: {total_text_length/successful_frames if successful_frames > 0 else 0:.1f} å­—ç¬¦")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_chatgpt_frame_strategy()